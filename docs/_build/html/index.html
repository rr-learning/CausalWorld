

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="python" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="python" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Welcome to Causal RL Bench&#39;s documentation! &mdash; tf_neuralmpc 1.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="#" class="icon icon-home"> tf_neuralmpc
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Welcome to Causal RL Bench's documentation!</a></li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">tf_neuralmpc</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>Welcome to Causal RL Bench's documentation!</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="welcome-to-causal-rl-bench-s-documentation">
<h1>Welcome to Causal RL Bench's documentation!<a class="headerlink" href="#welcome-to-causal-rl-bench-s-documentation" title="Permalink to this headline">¶</a></h1>
<p>Existing benchmarks in reinforcement learning cover a rich and diverse set of
environments and it has been shown that agents can be trained to
solve very challenging tasks. Nevertheless, it is a common problem in RL
that agents are poor at transferring their learned skills to different but
related environments that share a lot of common structure as agents are usually
evaluated on the training distribution itself and similarities to other
environments are ambiguous. We propose a novel benchmark by releasing X sets of
infinitely many fully parameterized training environments in a robotics setting
which are equipped with unique
sets of testing environments. These environments facilitate a precise evaluation
protocol to test generalisation and robustness capabilities of the acting agents
due to a reformulation of switching between environments through an intervention
on the generative causal model of the environments that allows to quantify the
amount of common shared structure. The skills to learn range from simple to
extremely challenging although the compositional nature of the environments
should allow to reuse previously learned more primitive skills along a
naturally emerging curriculum of tasks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">number_of_agents</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">single_env</span><span class="p">,</span> <span class="n">parallel_env</span> <span class="o">=</span> <span class="n">EnvironmentWrapper</span><span class="o">.</span><span class="n">make_standard_gym_env</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                                 <span class="n">num_of_agents</span><span class="o">=</span><span class="n">number_of_agents</span><span class="p">)</span>
<span class="n">my_runner</span> <span class="o">=</span> <span class="n">Runner</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="p">[</span><span class="n">single_env</span><span class="p">,</span> <span class="n">parallel_env</span><span class="p">],</span>
                <span class="n">log_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">num_of_agents</span><span class="o">=</span><span class="n">number_of_agents</span><span class="p">)</span>
<span class="n">mpc_controller</span> <span class="o">=</span> <span class="n">my_runner</span><span class="o">.</span><span class="n">make_mpc_policy</span><span class="p">(</span><span class="n">dynamics_function</span><span class="o">=</span><span class="n">PendulumTrueModel</span><span class="p">(),</span>
                                        <span class="n">state_reward_function</span><span class="o">=</span><span class="n">pendulum_state_reward_function</span><span class="p">,</span>
                                        <span class="n">actions_reward_function</span><span class="o">=</span><span class="n">pendulum_actions_reward_function</span><span class="p">,</span>
                                        <span class="n">planning_horizon</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                                        <span class="n">optimizer_name</span><span class="o">=</span><span class="s1">&#39;PI2&#39;</span><span class="p">,</span>
                                        <span class="n">true_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">current_obs</span> <span class="o">=</span> <span class="n">single_env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">current_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">current_obs</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                   <span class="p">(</span><span class="n">number_of_agents</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
 <span class="n">action_to_execute</span><span class="p">,</span> <span class="n">expected_obs</span><span class="p">,</span> <span class="n">expected_reward</span> <span class="o">=</span> <span class="n">mpc_controller</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">current_obs</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
 <span class="n">current_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">single_env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_to_execute</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
 <span class="n">current_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">current_obs</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                       <span class="p">(</span><span class="n">number_of_agents</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
 <span class="n">single_env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>



<span class="n">modules</span><span class="o">/</span><span class="n">causal_rl_bench</span><span class="o">.</span><span class="n">rst</span>
</pre></div>
</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
</ul>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Ossama Ahmed and Jonas Ruthfuss

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>