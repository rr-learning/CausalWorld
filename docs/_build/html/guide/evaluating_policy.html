

<!DOCTYPE html>
<html class="writer-html5" lang="python" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Evaluation Protocols &mdash; causal_rl_bench 1.0.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CausalRLBench" href="../modules/causal_rl_bench.html" />
    <link rel="prev" title="Task distributions" href="task_setups.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> causal_rl_bench
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="task_setups.html">Task distributions</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Evaluation Protocols</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#example">Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-default-protocols">Using default protocols</a></li>
<li class="toctree-l2"><a class="reference internal" href="#defining-a-customized-protocol">Defining a customized protocol</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules/causal_rl_bench.html">CausalRLBench</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">causal_rl_bench</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Evaluation Protocols</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/guide/evaluating_policy.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="evaluation-protocols">
<span id="evaluating-policy"></span><h1>Evaluation Protocols<a class="headerlink" href="#evaluation-protocols" title="Permalink to this headline">¶</a></h1>
<p>A central feature of CausalWorld is the explicit parametric formulation of environments
that allow for a precise evaluation of generalisation capabilities with respect to any of the
defining environment variables.</p>
<p>In order to evaluate a certain generalisation aspect a evaluation protocol needs to be defined.
Each evaluation protocol defines a set of episodes that might differ from each other with respect to the
generalisation aspect under consideration. If e.g. the agent is evaluated regarding success under different
masses each episodes is a counterfactual version of the other with the mass of a certain object being the
only difference. It is straight forward to define protocols for other variables such as colors, object sizes,
friction coefficients, different goals, different initial positions or even in-episode interventions aspect to
test robustness aspects. After the given set of environments in a protocol are being evaluated,
various aggregated success metrics are being computed based on the fractional reward profiles of each episode.
For now the following metrics are computes:
1. mean_last_fractional_success (which is the mean of the last fractional reward along all protocol episodes)
2. mean_full_integrated_fractional_success (which is the mean of the normalized average fractional reward along all protocol episodes)
3. mean_last_integrated_fractional_success (which is the mean of the normalized average fractional reward over the last 20 episode time steps along all protocol episodes)</p>
<p>Each of the environments variables has two associated sets of accessible non-overlapping spaces: A and B.
When use_train_space_only is activated for the CausalWorld Object during training all the variables
are only allowed to take values within space A, though they generally wont visit the entire space spanned
(In the extreme case of training on the default task only you will only train on one point in this space).
Training on multiple points of this space can be achieved by the help of a CurriculumWrapper.</p>
<p>Having an explicit split between spaces A and B allows to test generalisation capabilities towards values
that are more in line with the notions of interpolating (space A) and extrapolating (space B) generalisation.</p>
<div class="section" id="example">
<h2>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<p>As an introductory example we want to show you how you can quantify different generalisation aspects depending
on the curriculum exposed during training using the pushing task.
We trained two different agents: First, on the default task only without any type of randomization applied
between episodes. This means the agent always starts with the same initial tool_block pose and the same goal block
pose. Naturally we expect the agent to overfit to this goal over time. Second, we train an agent using a goal
randomization curriculum where we sample new goal poses within space A every two episodes. Still, we keep the
initial tool_block poses and everything else fixed during training. As can be seen from the animation below we
see that the former agent overfits to the single goal pose seen during training whereas the later can also push the
tool block towards other goal poses. Evaluating both agents using the protocols defined in the pushing benchmark
we can quantify and compare performance regarding different aspects in an explicit way. In this case we can see that
the second agent not only generalizes to goal_poses in space A but also to goal_poses in space B, though slightly worse.
Likewise we can see that the agents can generalize to new initial goal poses or in_episode poses interventions to name
just a few interesting observations. Left (no goal pose randomization) and right (goal pose randomization).</p>
<a class="reference internal image-reference" href="../_images/pushing_no_randomization.gif"><img alt="no goal pose randomization" class="align-left" src="../_images/pushing_no_randomization.gif" style="width: 320.0px; height: 269.0px;" /></a>
<a class="reference internal image-reference" href="../_images/pushing_randomization.gif"><img alt="goal pose randomization" class="align-left" src="../_images/pushing_randomization.gif" style="width: 320.0px; height: 269.0px;" /></a>
<a class="reference internal image-reference" href="../_images/radar_plots_mean_last_fractional_success.png"><img alt="mean last integrated fractional success, radar plot" class="align-center" src="../_images/radar_plots_mean_last_fractional_success.png" style="width: 630.0px; height: 630.0px;" /></a>
</div>
<div class="section" id="using-default-protocols">
<h2>Using default protocols<a class="headerlink" href="#using-default-protocols" title="Permalink to this headline">¶</a></h2>
<p>Below we show some demo code how you can systematically evaluate agents on a set
of different protocols and visualize the results in radar plots of different scores</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">causal_rl_bench.task_generators.task</span> <span class="kn">import</span> <span class="n">task_generator</span>
<span class="kn">from</span> <span class="nn">causal_rl_bench.envs.causalworld</span> <span class="kn">import</span> <span class="n">CausalWorld</span>
<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">PPO2</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.policies</span> <span class="kn">import</span> <span class="n">MlpPolicy</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common</span> <span class="kn">import</span> <span class="n">set_global_seeds</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.vec_env</span> <span class="kn">import</span> <span class="n">SubprocVecEnv</span>
<span class="kn">from</span> <span class="nn">causal_rl_bench.evaluation.evaluation</span> <span class="kn">import</span> <span class="n">EvaluationPipeline</span>
<span class="kn">import</span> <span class="nn">causal_rl_bench.evaluation.protocols</span> <span class="k">as</span> <span class="nn">protocols</span>


<span class="n">log_relative_path</span> <span class="o">=</span> <span class="s1">&#39;./pushing_policy_tutorial_1&#39;</span>


<span class="k">def</span> <span class="nf">_make_env</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_init</span><span class="p">():</span>
        <span class="n">task</span> <span class="o">=</span> <span class="n">task_generator</span><span class="p">(</span><span class="n">task_generator_id</span><span class="o">=</span><span class="s2">&quot;pushing&quot;</span><span class="p">)</span>
        <span class="n">env</span> <span class="o">=</span> <span class="n">CausalWorld</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span> <span class="n">enable_visualization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">seed</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">env</span>
    <span class="n">set_global_seeds</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_init</span>


<span class="k">def</span> <span class="nf">train_policy</span><span class="p">():</span>
    <span class="n">ppo_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.9988</span><span class="p">,</span>
                  <span class="s2">&quot;n_steps&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
                  <span class="s2">&quot;ent_coef&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                  <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
                  <span class="s2">&quot;vf_coef&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
                  <span class="s2">&quot;max_grad_norm&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
                  <span class="s2">&quot;lam&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
                  <span class="s2">&quot;nminibatches&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
                  <span class="s2">&quot;noptepochs&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
                  <span class="s2">&quot;cliprange&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
                  <span class="s2">&quot;tensorboard_log&quot;</span><span class="p">:</span> <span class="n">log_relative_path</span><span class="p">}</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">)</span>
    <span class="n">policy_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">act_fun</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">net_arch</span><span class="o">=</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">SubprocVecEnv</span><span class="p">([</span><span class="n">_make_env</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">PPO2</span><span class="p">(</span><span class="n">MlpPolicy</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">_init_setup_model</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">policy_kwargs</span><span class="o">=</span><span class="n">policy_kwargs</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">ppo_config</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                <span class="n">tb_log_name</span><span class="o">=</span><span class="s2">&quot;ppo2&quot;</span><span class="p">,</span>
                <span class="n">reset_num_timesteps</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">))</span>
    <span class="n">env</span><span class="o">.</span><span class="n">env_method</span><span class="p">(</span><span class="s2">&quot;save_world&quot;</span><span class="p">,</span> <span class="n">log_relative_path</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span>


<span class="k">def</span> <span class="nf">evaluate_trained_policy</span><span class="p">():</span>
    <span class="c1"># Load the PPO2 policy trained on the pushing task</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">PPO2</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">,</span> <span class="s1">&#39;model.zip&#39;</span><span class="p">))</span>

    <span class="c1"># define a method for the policy fn of your trained model</span>

    <span class="k">def</span> <span class="nf">policy_fn</span><span class="p">(</span><span class="n">obs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># pass the different protocols you&#39;d like to evaluate in the following</span>
    <span class="n">evaluator</span> <span class="o">=</span> <span class="n">EvaluationPipeline</span><span class="p">(</span><span class="n">evaluation_protocols</span><span class="o">=</span><span class="p">[</span><span class="n">protocols</span><span class="o">.</span><span class="n">GoalPosesOOD</span><span class="p">(),</span>
                                                         <span class="n">protocols</span><span class="o">.</span><span class="n">InitialPosesOOD</span><span class="p">(),</span>
                                                         <span class="n">protocols</span><span class="o">.</span><span class="n">InEpisodePosesChange</span><span class="p">()],</span>
                                   <span class="n">tracker_path</span><span class="o">=</span><span class="n">log_relative_path</span><span class="p">,</span>
                                   <span class="n">initial_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># For demonstration purposes we evaluate the policy on 10 per cent of the default number of episodes per protocol</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">evaluate_policy</span><span class="p">(</span><span class="n">policy_fn</span><span class="p">,</span> <span class="n">fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">evaluator</span><span class="o">.</span><span class="n">save_scores</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">train_policy</span><span class="p">()</span>
    <span class="n">evaluate_trained_policy</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">causal_rl_bench.evaluation.evaluation</span> <span class="kn">import</span> <span class="n">EvaluationPipeline</span>
<span class="kn">from</span> <span class="nn">causal_rl_bench.benchmark.benchmarks</span> <span class="kn">import</span> <span class="n">REACHING_BENCHMARK</span>
<span class="kn">from</span> <span class="nn">causal_rl_bench.task_generators.task</span> <span class="kn">import</span> <span class="n">task_generator</span>
<span class="kn">from</span> <span class="nn">causal_rl_bench.envs.causalworld</span> <span class="kn">import</span> <span class="n">CausalWorld</span>
<span class="kn">import</span> <span class="nn">causal_rl_bench.evaluation.visualization.visualiser</span> <span class="k">as</span> <span class="nn">vis</span>

<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">PPO2</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.policies</span> <span class="kn">import</span> <span class="n">MlpPolicy</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common</span> <span class="kn">import</span> <span class="n">set_global_seeds</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.vec_env</span> <span class="kn">import</span> <span class="n">SubprocVecEnv</span>

<span class="n">log_relative_path</span> <span class="o">=</span> <span class="s1">&#39;./pushing_policy_tutorial_2&#39;</span>


<span class="k">def</span> <span class="nf">_make_env</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_init</span><span class="p">():</span>
        <span class="n">task</span> <span class="o">=</span> <span class="n">task_generator</span><span class="p">(</span><span class="n">task_generator_id</span><span class="o">=</span><span class="s2">&quot;reaching&quot;</span><span class="p">)</span>
        <span class="n">env</span> <span class="o">=</span> <span class="n">CausalWorld</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span> <span class="n">enable_visualization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">seed</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">env</span>
    <span class="n">set_global_seeds</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_init</span>


<span class="k">def</span> <span class="nf">train_policy</span><span class="p">():</span>
    <span class="n">ppo_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.9988</span><span class="p">,</span>
                  <span class="s2">&quot;n_steps&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
                  <span class="s2">&quot;ent_coef&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                  <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
                  <span class="s2">&quot;vf_coef&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
                  <span class="s2">&quot;max_grad_norm&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
                  <span class="s2">&quot;lam&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
                  <span class="s2">&quot;nminibatches&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
                  <span class="s2">&quot;noptepochs&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
                  <span class="s2">&quot;cliprange&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
                  <span class="s2">&quot;tensorboard_log&quot;</span><span class="p">:</span> <span class="n">log_relative_path</span><span class="p">}</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">)</span>
    <span class="n">policy_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">act_fun</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">net_arch</span><span class="o">=</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">SubprocVecEnv</span><span class="p">([</span><span class="n">_make_env</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">PPO2</span><span class="p">(</span><span class="n">MlpPolicy</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">_init_setup_model</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">policy_kwargs</span><span class="o">=</span><span class="n">policy_kwargs</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">ppo_config</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                <span class="n">tb_log_name</span><span class="o">=</span><span class="s2">&quot;ppo2&quot;</span><span class="p">,</span>
                <span class="n">reset_num_timesteps</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">))</span>
    <span class="n">env</span><span class="o">.</span><span class="n">env_method</span><span class="p">(</span><span class="s2">&quot;save_world&quot;</span><span class="p">,</span> <span class="n">log_relative_path</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span>


<span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">():</span>
    <span class="c1"># Load the PPO2 policy trained on the reaching task</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">PPO2</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">,</span> <span class="s1">&#39;model.zip&#39;</span><span class="p">))</span>
    <span class="c1"># define a method for the policy fn of your trained model</span>

    <span class="k">def</span> <span class="nf">policy_fn</span><span class="p">(</span><span class="n">obs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Let&#39;s evaluate the policy on some default evaluation protocols for reaching task</span>
    <span class="n">evaluation_protocols</span> <span class="o">=</span> <span class="n">REACHING_BENCHMARK</span><span class="p">[</span><span class="s1">&#39;evaluation_protocols&#39;</span><span class="p">]</span>

    <span class="n">evaluator</span> <span class="o">=</span> <span class="n">EvaluationPipeline</span><span class="p">(</span><span class="n">evaluation_protocols</span><span class="o">=</span>
                                   <span class="n">evaluation_protocols</span><span class="p">,</span>
                                   <span class="n">tracker_path</span><span class="o">=</span><span class="n">log_relative_path</span><span class="p">,</span>
                                   <span class="n">initial_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># For demonstration purposes we evaluate the policy on 10 per cent of the default number of episodes per protocol</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">evaluate_policy</span><span class="p">(</span><span class="n">policy_fn</span><span class="p">,</span> <span class="n">fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">evaluator</span><span class="o">.</span><span class="n">save_scores</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">)</span>
    <span class="n">experiments</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;reaching_model&#39;</span><span class="p">:</span> <span class="n">scores</span><span class="p">}</span>
    <span class="n">vis</span><span class="o">.</span><span class="n">generate_visual_analysis</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">,</span> <span class="n">experiments</span><span class="o">=</span><span class="n">experiments</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1">#first train the policy, skip if u already trained the policy</span>
    <span class="n">train_policy</span><span class="p">()</span>
    <span class="n">evaluate_model</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">causal_rl_bench.evaluation.evaluation</span> <span class="kn">import</span> <span class="n">EvaluationPipeline</span>
<span class="kn">from</span> <span class="nn">causal_rl_bench.benchmark.benchmarks</span> <span class="kn">import</span> <span class="n">PUSHING_BENCHMARK</span>
<span class="kn">from</span> <span class="nn">causal_rl_bench.task_generators.task</span> <span class="kn">import</span> <span class="n">task_generator</span>
<span class="kn">from</span> <span class="nn">causal_rl_bench.envs.causalworld</span> <span class="kn">import</span> <span class="n">CausalWorld</span>
<span class="kn">import</span> <span class="nn">causal_rl_bench.evaluation.visualization.visualiser</span> <span class="k">as</span> <span class="nn">vis</span>

<span class="kn">from</span> <span class="nn">stable_baselines</span> <span class="kn">import</span> <span class="n">PPO2</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.policies</span> <span class="kn">import</span> <span class="n">MlpPolicy</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common</span> <span class="kn">import</span> <span class="n">set_global_seeds</span>
<span class="kn">from</span> <span class="nn">stable_baselines.common.vec_env</span> <span class="kn">import</span> <span class="n">SubprocVecEnv</span>

<span class="n">log_relative_path</span> <span class="o">=</span> <span class="s1">&#39;./pushing_policy_tutorial_3&#39;</span>


<span class="k">def</span> <span class="nf">_make_env</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_init</span><span class="p">():</span>
        <span class="n">task</span> <span class="o">=</span> <span class="n">task_generator</span><span class="p">(</span><span class="n">task_generator_id</span><span class="o">=</span><span class="s2">&quot;pushing&quot;</span><span class="p">)</span>
        <span class="n">env</span> <span class="o">=</span> <span class="n">CausalWorld</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span> <span class="n">enable_visualization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">seed</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">env</span>
    <span class="n">set_global_seeds</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_init</span>


<span class="k">def</span> <span class="nf">train_policy</span><span class="p">():</span>
    <span class="n">ppo_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.9988</span><span class="p">,</span>
                  <span class="s2">&quot;n_steps&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
                  <span class="s2">&quot;ent_coef&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                  <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
                  <span class="s2">&quot;vf_coef&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>
                  <span class="s2">&quot;max_grad_norm&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
                  <span class="s2">&quot;lam&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
                  <span class="s2">&quot;nminibatches&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
                  <span class="s2">&quot;noptepochs&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
                  <span class="s2">&quot;cliprange&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
                  <span class="s2">&quot;tensorboard_log&quot;</span><span class="p">:</span> <span class="n">log_relative_path</span><span class="p">}</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">)</span>
    <span class="n">policy_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">act_fun</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">net_arch</span><span class="o">=</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">SubprocVecEnv</span><span class="p">([</span><span class="n">_make_env</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">PPO2</span><span class="p">(</span><span class="n">MlpPolicy</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">_init_setup_model</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">policy_kwargs</span><span class="o">=</span><span class="n">policy_kwargs</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">ppo_config</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                <span class="n">tb_log_name</span><span class="o">=</span><span class="s2">&quot;ppo2&quot;</span><span class="p">,</span>
                <span class="n">reset_num_timesteps</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">))</span>
    <span class="n">env</span><span class="o">.</span><span class="n">env_method</span><span class="p">(</span><span class="s2">&quot;save_world&quot;</span><span class="p">,</span> <span class="n">log_relative_path</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span>


<span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">():</span>
    <span class="c1"># Load the PPO2 policy trained on the pushing task</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">PPO2</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">,</span> <span class="s1">&#39;model.zip&#39;</span><span class="p">))</span>
    <span class="c1"># define a method for the policy fn of your trained model</span>

    <span class="k">def</span> <span class="nf">policy_fn</span><span class="p">(</span><span class="n">obs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Let&#39;s evaluate the policy on some default evaluation protocols for reaching task</span>
    <span class="n">evaluation_protocols</span> <span class="o">=</span> <span class="n">PUSHING_BENCHMARK</span><span class="p">[</span><span class="s1">&#39;evaluation_protocols&#39;</span><span class="p">]</span>

    <span class="n">evaluator</span> <span class="o">=</span> <span class="n">EvaluationPipeline</span><span class="p">(</span><span class="n">evaluation_protocols</span><span class="o">=</span>
                                   <span class="n">evaluation_protocols</span><span class="p">,</span>
                                   <span class="n">tracker_path</span><span class="o">=</span><span class="n">log_relative_path</span><span class="p">,</span>
                                   <span class="n">initial_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># For demonstration purposes we evaluate the policy on 10 per cent of the default number of episodes per protocol</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">evaluate_policy</span><span class="p">(</span><span class="n">policy_fn</span><span class="p">,</span> <span class="n">fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">evaluator</span><span class="o">.</span><span class="n">save_scores</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">)</span>
    <span class="n">experiments</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pushing_model&#39;</span><span class="p">:</span> <span class="n">scores</span><span class="p">}</span>
    <span class="n">vis</span><span class="o">.</span><span class="n">generate_visual_analysis</span><span class="p">(</span><span class="n">log_relative_path</span><span class="p">,</span> <span class="n">experiments</span><span class="o">=</span><span class="n">experiments</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1">#first train the policy, skip if u already trained the policy</span>
    <span class="n">train_policy</span><span class="p">()</span>
    <span class="n">evaluate_model</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="defining-a-customized-protocol">
<h2>Defining a customized protocol<a class="headerlink" href="#defining-a-customized-protocol" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../modules/causal_rl_bench.html" class="btn btn-neutral float-right" title="CausalRLBench" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="task_setups.html" class="btn btn-neutral float-left" title="Task distributions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Ossama Ahmed and Frederik Trauble

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>